import os
import re
import json
import requests
from bs4 import BeautifulSoup

WEEKDAYS = ['monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday']
FILENAME = "A2025_7_animes"
url = "https://yuc.wiki/202507"


def find_weekday_for_block(block):
    """å‘å‰æŸ¥æ‰¾æœ€è¿‘çš„æ˜ŸæœŸæ ‡è¯†"""
    current = block

    # å‘å‰æŸ¥æ‰¾å‰é¢çš„å…„å¼Ÿå…ƒç´ ï¼Œå¯»æ‰¾åŒ…å«æ˜ŸæœŸä¿¡æ¯çš„div
    while current:
        # æŸ¥æ‰¾å‰ä¸€ä¸ªå…„å¼Ÿå…ƒç´ 
        prev_sibling = current.find_previous_sibling()
        if prev_sibling:
            # åœ¨å‰ä¸€ä¸ªå…ƒç´ ä¸­æŸ¥æ‰¾ td.date2
            date_td = prev_sibling.find('td', class_='date2')
            if date_td:
                date_text = date_td.get_text(strip=True)
                weekday = parse_weekday_text(date_text)
                if weekday != "unknown":
                    return weekday
            current = prev_sibling
        else:
            # å¦‚æœæ²¡æœ‰å‰ä¸€ä¸ªå…„å¼Ÿå…ƒç´ ï¼Œå‘ä¸ŠæŸ¥æ‰¾çˆ¶å…ƒç´ 
            parent = current.find_parent()
            if parent and parent.name != 'html':
                current = parent
            else:
                break

    return "unknown"


def parse_weekday_text(text):
    """ä»æ–‡æœ¬ä¸­è§£ææ˜ŸæœŸå‡ """
    weekday_map = {
        'å‘¨ä¸€': 'monday', 'æœˆ': 'monday',
        'å‘¨äºŒ': 'tuesday', 'ç«': 'tuesday',
        'å‘¨ä¸‰': 'wednesday', 'æ°´': 'wednesday',
        'å‘¨å››': 'thursday', 'æœ¨': 'thursday',
        'å‘¨äº”': 'friday', 'é‡‘': 'friday',
        'å‘¨å…­': 'saturday', 'åœŸ': 'saturday',
        'å‘¨æ—¥': 'sunday', 'å‘¨å¤©': 'sunday', 'æ—¥': 'sunday'
    }

    for key, value in weekday_map.items():
        if key in text:
            return value

    return "unknown"


def crawl_yuc_online(url):
    headers = {
        "User-Agent": "Mozilla/5.0"
    }
    response = requests.get(url, headers=headers, timeout=10)
    response.raise_for_status()
    soup = BeautifulSoup(response.text, 'html.parser')

    weekly_data = {day: [] for day in WEEKDAYS}
    anime_blocks = soup.find_all('div', class_='div_date')

    for block in anime_blocks:
        try:
            # æ—¶é—´ & é›†æ•° - æ·»åŠ ç©ºå€¼æ£€æŸ¥
            time_elem = block.find('p', class_='imgtext4')
            time_text = time_elem.get_text(strip=True) if time_elem else "æœªçŸ¥æ—¶é—´"

            episode_elem = block.find('p', class_='imgep')
            episode_info = episode_elem.get_text(strip=True) if episode_elem else "æœªçŸ¥é›†æ•°"

            # å›¾ç‰‡é“¾æ¥ - æ·»åŠ ç©ºå€¼æ£€æŸ¥
            img_tag = block.find('img')
            image_url = ""
            if img_tag:
                image_url = img_tag.get('data-src') or img_tag.get('src') or ""
                if image_url.startswith('//'):
                    image_url = 'https:' + image_url

            # åŠ¨ç”»ä¿¡æ¯åœ¨åé¢div - æ·»åŠ ç©ºå€¼æ£€æŸ¥
            next_div = block.find_next_sibling('div')
            if not next_div:
                print(f"âš ï¸ æœªæ‰¾åˆ°åç»­divï¼Œè·³è¿‡æ­¤æ¡ç›®")
                continue

            # æ ¹æ®ä½ æä¾›çš„ç»“æ„ï¼Œæ ‡é¢˜åœ¨table->tr->tdä¸­
            title_td = next_div.find('td', class_=re.compile(r'date_title'))
            if not title_td:
                # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå°è¯•å…¶ä»–å¯èƒ½çš„classåç§°
                title_td = next_div.find('td', class_='date_title_')

            if not title_td:
                print(f"âš ï¸ æœªæ‰¾åˆ°æ ‡é¢˜å…ƒç´ ï¼Œè·³è¿‡æ­¤æ¡ç›®")
                continue

            # åŠ¨ç”»æ ‡é¢˜ï¼ˆå¯èƒ½æœ‰é“¾æ¥ï¼‰
            link_tag = title_td.find('a')
            title = link_tag.get_text(strip=True) if link_tag else title_td.get_text(strip=True)
            title = title.replace('\n', ' ').replace('<br>', ' ').strip()  # å¤„ç†æ¢è¡Œ

            if not title:
                print(f"âš ï¸ æ ‡é¢˜ä¸ºç©ºï¼Œè·³è¿‡æ­¤æ¡ç›®")
                continue

            # æ˜ŸæœŸåˆ¤æ–­ - å‘å‰æŸ¥æ‰¾æœ€è¿‘çš„æ˜ŸæœŸæ ‡è¯†
            weekday = find_weekday_for_block(block)
            if weekday == "unknown":
                print(f"âš ï¸ æœªèƒ½ç¡®å®šæ˜ŸæœŸä¿¡æ¯: {title}")

            # å›¾ç‰‡ä¿å­˜
            local_image = download_image(image_url, title) if image_url else ""

            # æ„é€ æ¡ç›®
            entry = {
                "title": title,
                "time": time_text,
                "episodes": episode_info,
                "image": local_image,
                "god": "å­—å¹•ç»„:",
                "ladder": "no",
                "url": "",
                "isWatch":"no"
            }

            if weekday in WEEKDAYS:
                weekly_data[weekday].append(entry)
            else:
                # å¦‚æœæ— æ³•ç¡®å®šæ˜ŸæœŸï¼Œæ”¾å…¥ä¸€ä¸ªé»˜è®¤åˆ†ç±»
                if 'unknown' not in weekly_data:
                    weekly_data['unknown'] = []
                weekly_data['unknown'].append(entry)

            print(f"âœ… æˆåŠŸè§£æ: {title} - {weekday}")

        except Exception as e:
            print(f"âŒ åŠ¨ç”»è§£æå¤±è´¥ï¼š{e}")
            # æ‰“å°æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯ç”¨äºè°ƒè¯•
            import traceback
            traceback.print_exc()

    return weekly_data


def download_image(url, title):
    try:
        if not url:
            return ""
        os.makedirs(f'data/{FILENAME}/images', exist_ok=True)
        # æ›´å®‰å…¨çš„æ–‡ä»¶åå¤„ç†
        safe_title = re.sub(r'[^\w\s-]', '', title.replace(' ', '_'))[:30]
        if not safe_title:  # å¦‚æœå¤„ç†åæ–‡ä»¶åä¸ºç©º
            safe_title = "untitled"
        filepath = f"data/{FILENAME}/images/{safe_title}.jpg"

        if os.path.exists(filepath):
            return filepath

        response = requests.get(url, timeout=10)
        response.raise_for_status()
        with open(filepath, 'wb') as f:
            f.write(response.content)

        print(f"âœ… å·²ä¿å­˜å›¾ç‰‡: {filepath}")
        return filepath
    except Exception as e:
        print(f"âŒ ä¸‹è½½å›¾ç‰‡å¤±è´¥ï¼š{url} - {e}")
        return ""


def save_json(data, filename=f"data/{FILENAME}/{FILENAME}.json"):
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=4)
    print(f"âœ… å·²ä¿å­˜ä¿¡æ¯åˆ° {filename}")


def main():
    data = crawl_yuc_online(url)
    save_json(data)

    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    total_count = sum(len(animes) for animes in data.values())
    print(f"\nğŸ“Š çˆ¬å–ç»Ÿè®¡:")
    for day, animes in data.items():
        if animes:
            print(f"  {day}: {len(animes)} éƒ¨åŠ¨ç”»")
    print(f"  æ€»è®¡: {total_count} éƒ¨åŠ¨ç”»")


if __name__ == '__main__':
    main()
